{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f533062a",
   "metadata": {},
   "source": [
    "# ü©∫ Clasificaci√≥n de Melanomas a partir de Im√°genes\n",
    "\n",
    "## üåë Introducci√≥n\n",
    "El **melanoma** es un tipo de c√°ncer de piel que se origina en los **melanocitos**, las c√©lulas encargadas de producir la melanina, el pigmento que da color a la piel, ojos y cabello.  \n",
    "Aunque representa un porcentaje peque√±o de los c√°nceres de piel, es el **m√°s agresivo** debido a su gran capacidad para invadir otros tejidos y producir met√°stasis.  \n",
    "La detecci√≥n temprana es fundamental, ya que aumenta considerablemente las posibilidades de un tratamiento exitoso y de supervivencia del paciente.  \n",
    "\n",
    "---\n",
    "\n",
    "## üîé Metodolog√≠a cl√≠nica: Regla ABCD\n",
    "En dermatolog√≠a se utiliza la **regla ABCD** como gu√≠a visual para identificar posibles melanomas:  \n",
    "\n",
    "- **A ‚Äì Asimetr√≠a**: los melanomas suelen tener formas irregulares; un lunar benigno suele ser sim√©trico.  \n",
    "- **B ‚Äì Bordes**: los melanomas presentan bordes difusos, dentados o irregulares, frente a los bordes definidos de los lunares normales.  \n",
    "- **C ‚Äì Color**: los melanomas pueden mostrar m√∫ltiples tonalidades (negro, marr√≥n, rojo, azul, blanco), mientras que los lunares benignos suelen ser uniformes.  \n",
    "- **D ‚Äì Di√°metro**: lesiones con un di√°metro superior a 6 mm se consideran sospechosas.  \n",
    "\n",
    "A esta regla a veces se a√±ade la **E ‚Äì Evoluci√≥n**, que hace referencia a cambios en el tama√±o, forma, color o la aparici√≥n de s√≠ntomas (picor, sangrado).  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objetivo del trabajo\n",
    "El objetivo de este proyecto es **desarrollar un modelo de aprendizaje autom√°tico capaz de clasificar im√°genes de lesiones cut√°neas** para distinguir entre melanomas y lesiones benignas.  \n",
    "Para ello:  \n",
    "- Se trabajar√° con un conjunto de im√°genes m√©dicas.  \n",
    "- Se aplicar√° un flujo de trabajo basado en la preparaci√≥n y preprocesamiento de datos, entrenamiento de modelos de deep learning y an√°lisis de resultados.  \n",
    "- Se evaluar√° el desempe√±o de los modelos con m√©tricas adecuadas, y se comparar√° con la metodolog√≠a cl√≠nica tradicional basada en la regla ABCD.  \n",
    "\n",
    "Este proyecto busca **apoyar el diagn√≥stico m√©dico mediante t√©cnicas computacionales**, aportando una herramienta complementaria a la pr√°ctica cl√≠nica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb742632",
   "metadata": {},
   "source": [
    "# IMPORTACIONES Y RUTAS NECESARIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be5a267",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1455] El archivo de paginaci√≥n es demasiado peque√±o para completar la operaci√≥n. Error loading \"d:\\proyectos\\Caso_aprendizaje-Melanomas\\.venv\\lib\\site-packages\\torch\\lib\\cublas64_12.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_grid\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, ImageOps\n",
      "File \u001b[1;32md:\\proyectos\\Caso_aprendizaje-Melanomas\\.venv\\lib\\site-packages\\torchvision\\__init__.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodulefinder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n",
      "File \u001b[1;32md:\\proyectos\\Caso_aprendizaje-Melanomas\\.venv\\lib\\site-packages\\torch\\__init__.py:262\u001b[0m\n\u001b[0;32m    258\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    260\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 262\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\proyectos\\Caso_aprendizaje-Melanomas\\.venv\\lib\\site-packages\\torch\\__init__.py:245\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    241\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    242\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    244\u001b[0m     )\n\u001b[1;32m--> 245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1455] El archivo de paginaci√≥n es demasiado peque√±o para completar la operaci√≥n. Error loading \"d:\\proyectos\\Caso_aprendizaje-Melanomas\\.venv\\lib\\site-packages\\torch\\lib\\cublas64_12.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "# IMPORTACIONES\n",
    "# Standard library\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "from skimage import color, filters, morphology, util\n",
    "\n",
    "# Third-party\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "\n",
    "# RUTAS\n",
    "\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "DATA_DIR = Path(\"D:/proyectos/Caso_aprendizaje-Melanomas/data/raw\")\n",
    "SPLITS = [\"train\", \"val\", \"test\"]\n",
    "ALLOWED_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "TRAIN_DIR = DATA_DIR / \"train\"\n",
    "VAL_DIR   = DATA_DIR / \"val\"\n",
    "TEST_DIR  = DATA_DIR / \"test\"\n",
    "QUAR_DIR = DATA_DIR / \"_quarantine\"\n",
    "QUAR_DIR.mkdir(exist_ok=True)\n",
    "for p in (TRAIN_DIR, VAL_DIR, TEST_DIR): print(p, \"exists:\", p.exists())\n",
    "\n",
    "# Hiperpar√°metros\n",
    "CLASSES = 2\n",
    "BATCH   = 32\n",
    "ROWS = COLS = 224\n",
    "INPUT_CH = 3\n",
    "EPOCHS  = 20\n",
    "TEST_MAX_SAMPLES = 3000\n",
    "SEED    = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd1f98",
   "metadata": {},
   "source": [
    "## üìå ¬øPor qu√© separar en *train*, *validation* y *test*?\n",
    "\n",
    "En proyectos de *Machine Learning* es fundamental dividir los datos en diferentes conjuntos:\n",
    "\n",
    "- **Train (entrenamiento)**  \n",
    "  Se utiliza para que el modelo aprenda los patrones de las im√°genes.  \n",
    "\n",
    "- **Validation (validaci√≥n)**  \n",
    "  Sirve para ajustar hiperpar√°metros (n√∫mero de capas, tasa de aprendizaje, etc.) y comparar modelos sin mirar el *test*.  \n",
    "  üëâ Es una especie de \"examen parcial\": nos avisa si el modelo se est√° sobreajustando (*overfitting*) o si generaliza bien.  \n",
    "\n",
    "- **Test (prueba final)**  \n",
    "  Es el conjunto que se mantiene completamente aislado durante todo el desarrollo.  \n",
    "  üëâ Representa el \"examen final\" y nos da una estimaci√≥n real del rendimiento del modelo en datos nunca vistos.  \n",
    "\n",
    "‚úÖ En este caso (clasificaci√≥n de melanomas), separar en *train* y *val* es crucial porque:  \n",
    "- Ayuda a evitar *overfitting*, que ser√≠a muy peligroso si el modelo solo memoriza im√°genes.  \n",
    "- Permite evaluar de manera m√°s realista la capacidad de generalizaci√≥n antes de aplicarlo en un entorno cl√≠nico.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9444ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Benign</th>\n",
       "      <th>Malignant</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>5346</td>\n",
       "      <td>4752</td>\n",
       "      <td>10098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>943</td>\n",
       "      <td>838</td>\n",
       "      <td>1781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Benign  Malignant  Total\n",
       "train    5346       4752  10098\n",
       "val       943        838   1781\n",
       "test     1000       1000   2000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Funci√≥n para contar im√°genes por carpeta\n",
    "def count_images_in_dir(dir_path: Path):\n",
    "    if not dir_path.exists():\n",
    "        return {}\n",
    "    counts = {}\n",
    "    for cls_dir in dir_path.iterdir():\n",
    "        if cls_dir.is_dir():\n",
    "            n_imgs = sum(1 for f in cls_dir.rglob(\"*\") if f.suffix.lower() in ALLOWED_EXTS)\n",
    "            counts[cls_dir.name] = n_imgs\n",
    "    return counts\n",
    "\n",
    "# Recolectar conteos\n",
    "all_counts = {}\n",
    "for split in SPLITS:\n",
    "    split_dir = DATA_DIR / split\n",
    "    all_counts[split] = count_images_in_dir(split_dir)\n",
    "\n",
    "# Mostrar tabla\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_counts).fillna(0).astype(int).T\n",
    "df[\"Total\"] = df.sum(axis=1)\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c33059",
   "metadata": {},
   "source": [
    "## ‚ö° Uso de GPU en PyTorch\n",
    "\n",
    "Para acelerar el entrenamiento de los modelos, es importante usar la **GPU** en lugar de la **CPU** siempre que est√© disponible.  \n",
    "\n",
    "Con PyTorch podemos:  \n",
    "- Detectar autom√°ticamente si el sistema tiene una GPU con CUDA.  \n",
    "- Mover el **modelo** y los **tensores** al dispositivo adecuado (`cuda` o `cpu`).  \n",
    "- Asegurarnos de que todo el flujo de entrenamiento (inputs, labels, modelo, p√©rdidas) se ejecute en la GPU.  \n",
    "\n",
    "El siguiente c√≥digo hace esta detecci√≥n y muestra qu√© dispositivo se est√° utilizando.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91ce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible: True\n",
      "GPU: NVIDIA GeForce MX250\n",
      "Tensor en: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# prueba r√°pida en GPU\n",
    "x = torch.randn(4096, 4096, device=device)\n",
    "y = x @ x\n",
    "print(\"Tensor en:\", y.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278bd94",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Dimensionado consistente de im√°genes\n",
    "\n",
    "Para que el modelo funcione con cualquier imagen, todas deben tener **mismo tama√±o** y **misma normalizaci√≥n**.\n",
    "\n",
    "- Tama√±o objetivo: `224x224` (est√°ndar para muchas CNN).\n",
    "- Estrategia:\n",
    "  - **Entrenamiento**: redimensionar + (opcional) aumentos de datos ligeros.\n",
    "  - **Validaci√≥n/Test**: solo redimensionar (sin aumentos).\n",
    "  - **Inferencia**: usar **exactamente** la misma trasformaci√≥n que `val/test`.\n",
    "\n",
    "> Nota: es preferible **transformar al vuelo** (con `DataLoader`) y **no sobrescribir** las im√°genes en disco. Si quieres un *export* a `data/processed/`, al final tienes una celda opcional para guardar las redimensionadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b02cd49",
   "metadata": {},
   "source": [
    "## A) Limpieza r√°pida (corruptas + orientaci√≥n + RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe099c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 10098 imgs, movidas a cuarentena: 0\n",
      "val: 1781 imgs, movidas a cuarentena: 0\n",
      "test: 2000 imgs, movidas a cuarentena: 0\n",
      "‚úÖ Limpieza b√°sica terminada (sin modificar im√°genes buenas).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clean_folder(split_dir: Path):\n",
    "    n_total=n_quar=0\n",
    "    for p in split_dir.rglob(\"*\"):\n",
    "        if p.suffix.lower() not in {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\"}:\n",
    "            continue\n",
    "        n_total += 1\n",
    "        try:\n",
    "            with Image.open(p) as im:\n",
    "                im = ImageOps.exif_transpose(im)   # respeta la orientaci√≥n EXIF\n",
    "                if im.mode != \"RGB\":\n",
    "                    im = im.convert(\"RGB\")         # fuerza 3 canales\n",
    "                # No se guarda en disco: solo validamos que abre bien\n",
    "        except Exception as e:\n",
    "            n_quar += 1\n",
    "            shutil.move(str(p), QUAR_DIR / p.name)\n",
    "    print(f\"{split_dir.name}: {n_total} imgs, movidas a cuarentena: {n_quar}\")\n",
    "\n",
    "for split in [\"train\",\"val\",\"test\"]:\n",
    "    d = DATA_DIR / split\n",
    "    if d.exists():\n",
    "        clean_folder(d)\n",
    "print(\"‚úÖ Limpieza b√°sica terminada (sin modificar im√°genes buenas).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8d561",
   "metadata": {},
   "source": [
    "## B) Transforms coherentes (letterbox + resize + tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4963b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class PadToSquare:\n",
    "    def __init__(self, fill=0): self.fill = fill\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        s = max(w, h)\n",
    "        pad_left  = (s - w) // 2\n",
    "        pad_right = s - w - pad_left\n",
    "        pad_top   = (s - h) // 2\n",
    "        pad_bottom= s - h - pad_top\n",
    "        return TF.pad(img, [pad_left, pad_top, pad_right, pad_bottom], fill=self.fill)\n",
    "\n",
    "# >>> Si vas a usar TRANSFER LEARNING (recomendado):\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    PadToSquare(fill=0),\n",
    "    transforms.Resize((ROWS, COLS)),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05,0.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "eval_tfms = transforms.Compose([\n",
    "    PadToSquare(fill=0),\n",
    "    transforms.Resize((ROWS, COLS)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "\n",
    "train_ds = datasets.ImageFolder(DATA_DIR / \"train\", transform=train_tfms)\n",
    "val_ds   = datasets.ImageFolder(DATA_DIR / \"val\",   transform=eval_tfms) if (DATA_DIR / \"val\").exists() else None\n",
    "test_ds  = datasets.ImageFolder(DATA_DIR / \"test\",  transform=eval_tfms) if (DATA_DIR / \"test\").exists() else None\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=0, pin_memory=(device.type==\"cuda\"))\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=(device.type==\"cuda\")) if val_ds else None\n",
    "test_dl  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=(device.type==\"cuda\")) if test_ds else None\n",
    "\n",
    "xb, yb = next(iter(train_dl))\n",
    "print(\"Batch:\", xb.shape)  # [B, 3, 224, 224]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc359fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1.1 Semillas fijas =====\n",
    "import torch, random, numpy as np\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1.2 EXIF dentro del transform =====\n",
    "from PIL import ImageOps\n",
    "class ExifTranspose:\n",
    "    def __call__(self, img):\n",
    "        return ImageOps.exif_transpose(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfabb066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1.3 Conteo por clase =====\n",
    "from collections import Counter\n",
    "def count_per_class(ds):\n",
    "    idxs = [y for _, y in ds.samples] if hasattr(ds, \"samples\") else [y for _, y in ds.imgs]\n",
    "    c = Counter(idxs)\n",
    "    classes = ds.classes\n",
    "    return {classes[i]: c.get(i,0) for i in range(len(classes))}\n",
    "\n",
    "print(\"Train:\", count_per_class(train_ds))\n",
    "if val_ds:  print(\"Val:\",   count_per_class(val_ds))\n",
    "if test_ds: print(\"Test:\",  count_per_class(test_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d813f7c6",
   "metadata": {},
   "source": [
    "## 2) Padding ‚Äúinteligente‚Äù (evitar bordes negros que meten sesgo)\n",
    "\n",
    "Tu PadToSquare(fill=0) pone bordes negros. En dermatoscop√≠a esos bordes pueden convertirse en se√±al espuria. Mejor:\n",
    "\n",
    "- Reflect (espejado) o\n",
    "\n",
    "- Rellenar con el color mediano del borde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c4d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "class PadToSquareReflect:\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        s = max(w, h)\n",
    "        pad_left  = (s - w) // 2\n",
    "        pad_right = s - w - pad_left\n",
    "        pad_top   = (s - h) // 2\n",
    "        pad_bottom= s - h - pad_top\n",
    "        return TF.pad(img, [pad_left, pad_top, pad_right, pad_bottom], padding_mode=\"reflect\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9fee1e",
   "metadata": {},
   "source": [
    "## 4) Augmentaciones acordes a dermatoscop√≠a\n",
    "\n",
    "- Rotaci√≥n: en lesiones de piel la orientaci√≥n no importa ‚Üí rotaciones moderadas (¬±15‚Äì30¬∫) OK.\n",
    "\n",
    "- Flip horizontal: suele estar bien. El flip vertical tambi√©n es razonable (criterio cl√≠nico: no hay ‚Äúarriba/abajo‚Äù).\n",
    "\n",
    "- Traslaci√≥n leve: ‚úî\n",
    "\n",
    "Evita cambios de color agresivos (jitter fuerte) si har√°s an√°lisis de color; si usas un modelo de clasificaci√≥n general, un ColorJitter suave puede ayudar, pero con cuidado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b10dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = transforms.Compose([\n",
    "    ExifTranspose(),\n",
    "    PadToSquareReflect(),\n",
    "    transforms.Resize((ROWS, COLS), antialias=True),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05,0.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),  # o DATA_MEAN/STD\n",
    "])\n",
    "\n",
    "eval_tfms = transforms.Compose([\n",
    "    ExifTranspose(),\n",
    "    PadToSquareReflect(),\n",
    "    transforms.Resize((ROWS, COLS), antialias=True),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c747b270",
   "metadata": {},
   "source": [
    "## 5) DataLoader estable y eficiente\n",
    "\n",
    "- En Windows, si num_workers>0, a√±ade persistent_workers=True para evitar re-forks.\n",
    "\n",
    "- Usa pin_memory=(device.type==\"cuda\").\n",
    "\n",
    "- Si ves cuellos de botella, sube a num_workers=2‚Äì4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 2\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n",
    "                      num_workers=workers, pin_memory=(device.type==\"cuda\"),\n",
    "                      persistent_workers=(workers>0))\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH, shuffle=False,\n",
    "                      num_workers=workers, pin_memory=(device.type==\"cuda\"),\n",
    "                      persistent_workers=(workers>0)) if val_ds else None\n",
    "test_dl  = DataLoader(test_ds, batch_size=BATCH, shuffle=False,\n",
    "                      num_workers=workers, pin_memory=(device.type==\"cuda\"),\n",
    "                      persistent_workers=(workers>0)) if test_ds else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f81a2",
   "metadata": {},
   "source": [
    "## 6) ‚ÄúSanity checks‚Äù visuales\n",
    "\n",
    "Antes de entrenar, mira un batch para comprobar que el padding/rotaci√≥n/color se ve razonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cacbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xb, yb = next(iter(train_dl))\n",
    "grid = make_grid(xb[:16], nrow=8, normalize=True)  # normalize=True solo para ver\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.imshow(grid.permute(1,2,0).cpu().numpy())\n",
    "plt.axis(\"off\"); plt.title(\"Muestra de augmentaciones (train)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f0e71b",
   "metadata": {},
   "source": [
    "## 8) Cosas espec√≠ficas de piel\n",
    "\n",
    "- Hair removal (eliminaci√≥n de pelos) o m√°scara r√°pida de lesi√≥n antes de redimensionar (evita que el fondo/piel dominen).\n",
    "\n",
    "- Color constancy (p. ej., Shades-of-Gray) si el color es determinante.\n",
    "\n",
    "- Recorte por m√°scara (crop alrededor de la lesi√≥n + padding reflect) para centrar la ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Celda √∫nica: Preprocesado dermatosc√≥pico (hair removal + color constancy + m√°scara + crop reflect) ===\n",
    "# Requisitos: pillow, numpy, opencv-python, scikit-image\n",
    "# pip install pillow numpy opencv-python scikit-image\n",
    "\n",
    "\n",
    "\n",
    "# ---------- 1) Hair removal (inpainting sobre trazos oscuros finos) ----------\n",
    "def hair_removal_bh_inpaint(rgb, se_size=9, thresh_percentile=90, dilate_iter=1, method=cv2.INPAINT_TELEA):\n",
    "    \"\"\"\n",
    "    Elimina pelos oscuros:\n",
    "    - Black-hat morfol√≥gico para detectar filamentos oscuros\n",
    "    - Umbral por percentil (robusto)\n",
    "    - Dilataci√≥n leve y 'inpainting' para rellenar\n",
    "    \"\"\"\n",
    "    bgr = rgb[:, :, ::-1].copy()  # PIL RGB -> OpenCV BGR\n",
    "    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # black-hat para resaltar trazos oscuros finos\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (se_size, se_size))\n",
    "    bh = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, kernel)\n",
    "\n",
    "    # umbral robusto por percentil para seleccionar pelos\n",
    "    thr = np.percentile(bh, thresh_percentile)\n",
    "    mask = (bh > thr).astype(np.uint8) * 255\n",
    "    if dilate_iter > 0:\n",
    "        mask = cv2.dilate(mask, np.ones((3,3), np.uint8), iterations=dilate_iter)\n",
    "\n",
    "    # inpainting\n",
    "    inpainted = cv2.inpaint(bgr, mask, 3, method)\n",
    "    out = inpainted[:, :, ::-1]  # back to RGB\n",
    "    return out\n",
    "\n",
    "# ---------- 2) Color constancy (Shades-of-Gray) ----------\n",
    "def shades_of_gray_cc(rgb, p=6, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Shades-of-Gray (p-norm). Normaliza ganancias de canal para reducir dominantes de iluminaci√≥n.\n",
    "    No cambia el contraste local, solo reescala canales.\n",
    "    \"\"\"\n",
    "    img = rgb.astype(np.float32) / 255.0\n",
    "    # evitar saturaciones por valores muy cercanos a 0\n",
    "    img = np.clip(img, 0.0, 1.0) + eps\n",
    "\n",
    "    # medias p-norm por canal\n",
    "    mean_p = (img ** p).mean(axis=(0, 1)) ** (1.0 / p)\n",
    "    # ganancias inversas normalizadas\n",
    "    gain = (1.0 / mean_p)\n",
    "    gain = gain / (np.linalg.norm(gain) + eps) * np.sqrt(3)\n",
    "\n",
    "    out = img * gain[None, None, :]\n",
    "    out = np.clip(out, 0.0, 1.0)\n",
    "    return (out * 255.0).astype(np.uint8)\n",
    "\n",
    "# ---------- 3) M√°scara r√°pida de la lesi√≥n (umbral en canal 'a' de CIELAB) ----------\n",
    "def quick_lesion_mask(rgb, min_obj=300, hole_area=800):\n",
    "    \"\"\"\n",
    "    Segmentaci√≥n r√°pida: umbral de Otsu sobre canal 'a' en CIELAB (con blur).\n",
    "    Limpieza morfol√≥gica ligera.\n",
    "    \"\"\"\n",
    "    lab = color.rgb2lab(rgb).astype(np.float32)\n",
    "    a = lab[:, :, 1]\n",
    "    a_blur = cv2.GaussianBlur(a, (5, 5), 0)\n",
    "    th = filters.threshold_otsu(a_blur)\n",
    "    mask = (a_blur > th)\n",
    "\n",
    "    # morfolog√≠a\n",
    "    mask = morphology.remove_small_holes(mask, area_threshold=hole_area)\n",
    "    mask = morphology.remove_small_objects(mask, min_size=min_obj)\n",
    "    mask = morphology.binary_opening(mask, morphology.disk(3))\n",
    "    mask = morphology.binary_closing(mask, morphology.disk(5))\n",
    "    return mask.astype(np.uint8)\n",
    "\n",
    "# ---------- 4) Crop por m√°scara + reflect pad a cuadrado + resize ----------\n",
    "def bbox_from_mask(mask, margin=0.10):\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    if len(xs) == 0 or len(ys) == 0:\n",
    "        return None\n",
    "    x1, x2 = xs.min(), xs.max()\n",
    "    y1, y2 = ys.min(), ys.max()\n",
    "    h, w = mask.shape\n",
    "    # margen relativo\n",
    "    dx = int((x2 - x1 + 1) * margin)\n",
    "    dy = int((y2 - y1 + 1) * margin)\n",
    "    x1 = max(0, x1 - dx); x2 = min(w - 1, x2 + dx)\n",
    "    y1 = max(0, y1 - dy); y2 = min(h - 1, y2 + dy)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def reflect_pad_to_square(img_np):\n",
    "    \"\"\"\n",
    "    Rellena por reflect hasta cuadrado, sin bordes negros.\n",
    "    img_np: RGB uint8 (H, W, 3)\n",
    "    \"\"\"\n",
    "    h, w = img_np.shape[:2]\n",
    "    if h == w:\n",
    "        return img_np\n",
    "    s = max(h, w)\n",
    "    pad_top = (s - h) // 2\n",
    "    pad_bottom = s - h - pad_top\n",
    "    pad_left = (s - w) // 2\n",
    "    pad_right = s - w - pad_left\n",
    "    return cv2.copyMakeBorder(\n",
    "        img_np, pad_top, pad_bottom, pad_left, pad_right,\n",
    "        borderType=cv2.BORDER_REFLECT_101\n",
    "    )\n",
    "\n",
    "def crop_by_mask_reflect_resize(rgb, mask, out_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    - recorta al bbox de la m√°scara (+margen interno en bbox_from_mask)\n",
    "    - reflect-pad hasta cuadrado\n",
    "    - resize final\n",
    "    \"\"\"\n",
    "    bbox = bbox_from_mask(mask, margin=0.10)\n",
    "    if bbox is not None:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        crop = rgb[y1:y2+1, x1:x2+1, :]\n",
    "        if crop.size == 0:\n",
    "            crop = rgb\n",
    "    else:\n",
    "        crop = rgb\n",
    "    sq = reflect_pad_to_square(crop)\n",
    "    # resize antialias\n",
    "    pil = Image.fromarray(sq)\n",
    "    pil = pil.resize(out_size, Image.Resampling.LANCZOS)\n",
    "    return np.asarray(pil)\n",
    "\n",
    "# ---------- 5) Transform clase: todo junto y configurable ----------\n",
    "class DermPreproc:\n",
    "    \"\"\"\n",
    "    Preprocesado dermatosc√≥pico para usar como transform inicial (antes de ToTensor):\n",
    "      - exif transpose\n",
    "      - hair removal (opcional)\n",
    "      - color constancy Shades-of-Gray (opcional)\n",
    "      - m√°scara r√°pida + crop por m√°scara + reflect pad\n",
    "      - resize final\n",
    "    Devuelve PIL.Image RGB.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 out_size=(224, 224),\n",
    "                 do_hair_removal=True,\n",
    "                 do_color_constancy=True,\n",
    "                 return_mask=False):\n",
    "        self.out_size = out_size\n",
    "        self.do_hair_removal = do_hair_removal\n",
    "        self.do_color_constancy = do_color_constancy\n",
    "        self.return_mask = return_mask\n",
    "\n",
    "    def __call__(self, pil_img):\n",
    "        # 1) Exif transpose y asegurar RGB\n",
    "        pil_img = ImageOps.exif_transpose(pil_img)\n",
    "        if pil_img.mode != \"RGB\":\n",
    "            pil_img = pil_img.convert(\"RGB\")\n",
    "\n",
    "        rgb = np.array(pil_img)\n",
    "\n",
    "        # 2) Hair removal\n",
    "        if self.do_hair_removal:\n",
    "            try:\n",
    "                rgb = hair_removal_bh_inpaint(rgb, se_size=9, thresh_percentile=90,\n",
    "                                              dilate_iter=1, method=cv2.INPAINT_TELEA)\n",
    "            except Exception:\n",
    "                # Si falla OpenCV o algo, seguimos sin HR\n",
    "                pass\n",
    "\n",
    "        # 3) Color constancy (Shades-of-Gray)\n",
    "        if self.do_color_constancy:\n",
    "            rgb = shades_of_gray_cc(rgb, p=6)\n",
    "\n",
    "        # 4) M√°scara r√°pida de lesi√≥n\n",
    "        try:\n",
    "            mask = quick_lesion_mask(rgb, min_obj=300, hole_area=800)\n",
    "        except Exception:\n",
    "            # Fallback: m√°scara vac√≠a (sin crop)\n",
    "            mask = np.zeros(rgb.shape[:2], dtype=np.uint8)\n",
    "\n",
    "        # 5) Crop + reflect pad + resize\n",
    "        rgb_out = crop_by_mask_reflect_resize(rgb, mask, out_size=self.out_size)\n",
    "\n",
    "        pil_out = Image.fromarray(rgb_out)\n",
    "        if self.return_mask:\n",
    "            # Redimensionamos la m√°scara al mismo tama√±o para inspecci√≥n/uso\n",
    "            m = Image.fromarray((mask*255).astype(np.uint8))\n",
    "            m = m.resize(self.out_size, Image.Resampling.NEAREST)\n",
    "            return pil_out, m\n",
    "        return pil_out\n",
    "\n",
    "# ---------- 6) Ejemplo de uso r√°pido ----------\n",
    "if __name__ == \"__main__\":\n",
    "    #Ruta de ejemplo (c√°mbiala por una tuya)\n",
    "    from pathlib import Path\n",
    "    img_path = Path(\"data/raw/train/Benign/8.jpg\")\n",
    "    pil = Image.open(img_path)\n",
    "    pre = DermPreproc(out_size=(224,224), do_hair_removal=True, do_color_constancy=True, return_mask=True)\n",
    "    img_proc, mask_proc = pre(pil)\n",
    "    img_proc.show(); mask_proc.show()\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Celda 1: Imports, rutas, device y parámetros ===\n",
    "from pathlib import Path\n",
    "import time, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# ---- Rutas (ajústalas si hace falta) ----\n",
    "DATA_DIR = Path(r\"D:/proyectos/Caso_aprendizaje-Melanomas/data/raw\")\n",
    "TRAIN_DIR = DATA_DIR / \"train\"\n",
    "VAL_DIR   = DATA_DIR / \"val\"\n",
    "TEST_DIR  = DATA_DIR / \"test\"\n",
    "\n",
    "for p in (TRAIN_DIR, VAL_DIR, TEST_DIR):\n",
    "    print(p, \"exists:\", p.exists())\n",
    "\n",
    "# ---- Semillas fijas ----\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ---- Device (GPU si hay) ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---- Parámetros de datos/entreno ----\n",
    "CLASSES = 2\n",
    "ROWS = COLS = 224\n",
    "BATCH = 16\n",
    "EPOCHS = 15\n",
    "LR = 1e-3\n",
    "NUM_WORKERS = 0            # En Windows, 0 es lo más estable. Si todo ok, prueba 2.\n",
    "USE_AMP = True             # Mixed precision en GPU; en CPU se ignora.\n",
    "\n",
    "# Normalización (ImageNet)\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262abd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Celda 2: Transforms (tratado de imagen) + DataLoaders ===\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((ROWS, COLS)),\n",
    "\n",
    "    # --- Augmentaciones ligeras ---\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15, fill=tuple(int(m*255) for m in IMAGENET_MEAN)),  # evita bordes negros\n",
    "\n",
    "    # --- Luz/contraste ---\n",
    "    transforms.ColorJitter(brightness=0.15, contrast=0.15),   # ±15%\n",
    "    transforms.RandomAutocontrast(p=0.3),\n",
    "\n",
    "    # --- Bordes/nitidez (suave) ---\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=1.5, p=0.3),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.8)),  # opcional, suaviza ruido fino\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize((ROWS, COLS)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "train_ds = datasets.ImageFolder(TRAIN_DIR, transform=train_tfms)\n",
    "val_ds   = datasets.ImageFolder(VAL_DIR,   transform=val_tfms)\n",
    "print(\"Clases:\", train_ds.classes)\n",
    "assert len(train_ds.classes) == CLASSES, f\"Esperaba {CLASSES} clases.\"\n",
    "\n",
    "# DataLoaders\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Celda 3: Definición del modelo CNN ===\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, in_ch=3, num_classes=2, drop_conv=0.20, drop_fc=0.5):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2,2), nn.Dropout(drop_conv),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2,2), nn.Dropout(drop_conv),\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2,2), nn.Dropout(drop_conv),\n",
    "        )\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2,2), nn.Dropout(drop_conv),\n",
    "        )\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(drop_fc),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)   # 224 -> 112\n",
    "        x = self.block2(x)   # 112 -> 56\n",
    "        x = self.block3(x)   # 56  -> 28\n",
    "        x = self.block4(x)   # 28  -> 14\n",
    "        x = self.gap(x)      # -> [B,256,1,1]\n",
    "        x = self.classifier(x)  # -> [B,2]\n",
    "        return x\n",
    "\n",
    "model = SmallCNN(in_ch=3, num_classes=CLASSES).to(device)\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Parámetros entrenables: {params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b7e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Celda 4: Loss, optimizador y métrica ===\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "def accuracy_from_logits(logits, targets):\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == targets).float().mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa59743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Celda 5: Entreno/validación ===\n",
    "scaler = torch.amp.GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16,\n",
    "                                enabled=USE_AMP and device.type == \"cuda\"):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc  += accuracy_from_logits(logits, y)\n",
    "        n += 1\n",
    "    return total_loss/n, total_acc/n\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        total_loss += loss.item()\n",
    "        total_acc  += accuracy_from_logits(logits, y)\n",
    "        n += 1\n",
    "    return total_loss/n, total_acc/n\n",
    "\n",
    "history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
    "BEST_PATH = \"smallcnn_simple_best.pth\"\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_dl, optimizer, criterion)\n",
    "    va_loss, va_acc = validate(model, val_dl, criterion)\n",
    "\n",
    "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"classes\": train_ds.classes,\n",
    "            \"img_size\": (ROWS, COLS)\n",
    "        }, BEST_PATH)\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(f\"[{epoch:02d}/{EPOCHS}] \"\n",
    "          f\"train_loss={tr_loss:.4f} acc={tr_acc:.4f} | \"\n",
    "          f\"val_loss={va_loss:.4f} acc={va_acc:.4f} | {dt:.1f}s\")\n",
    "\n",
    "print(\"Mejor modelo guardado en:\", BEST_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

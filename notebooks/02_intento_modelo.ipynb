{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4967edb",
   "metadata": {},
   "source": [
    "## üß† Arquitectura completa del modelo ‚Äî `SmallCNN_Res`\n",
    "\n",
    "Este modelo est√° formado por **bloques convolucionales residuales** que extraen caracter√≠sticas visuales a diferentes escalas,  \n",
    "y una **parte densa final (clasificador)** que convierte esas caracter√≠sticas en una decisi√≥n binaria (*Benign / Malignant*).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### üß© Arquitectura del modelo\n",
    "\n",
    "#### üñº Entrada  \n",
    "**Imagen RGB:** `[3 √ó 224 √ó 224]`\n",
    "\n",
    "---\n",
    "\n",
    "#### üß± Bloque 1 ‚Äî ResidualBlock(3 ‚Üí 64)\n",
    "- Conv2d(3, 64, kernel_size=3, padding=1)  \n",
    "- BatchNorm2d(64)  \n",
    "- SiLU  \n",
    "- Conv2d(64, 64, kernel_size=3, padding=1)  \n",
    "- BatchNorm2d(64)  \n",
    "- SiLU  \n",
    "- Dropout2d(0.20)  \n",
    "- Residual (1√ó1 conv si cambia canales)  \n",
    "- SiLU  \n",
    "- MaxPool2d(2,2)  \n",
    "üìè **Salida:** `[64 √ó 112 √ó 112]`\n",
    "\n",
    "---\n",
    "\n",
    "#### üß± Bloque 2 ‚Äî ResidualBlock(64 ‚Üí 128)\n",
    "- Conv2d(64, 128, kernel_size=3, padding=1)  \n",
    "- BatchNorm2d(128)  \n",
    "- SiLU  \n",
    "- Conv2d(128, 128, kernel_size=3, padding=1)  \n",
    "- BatchNorm2d(128)  \n",
    "- SiLU  \n",
    "- Dropout2d(0.20)  \n",
    "- Residual (1√ó1 conv)  \n",
    "- SiLU  \n",
    "- MaxPool2d(2,2)  \n",
    "üìè **Salida:** `[128 √ó 56 √ó 56]`\n",
    "\n",
    "---\n",
    "\n",
    "#### üß± Bloque 3 ‚Äî ResidualBlock(128 ‚Üí 256)\n",
    "- Conv2d(128, 256, kernel_size=3, padding=1)  \n",
    "- BatchNorm2d(256)  \n",
    "- SiLU  \n",
    "- Conv2d(256, 256, kernel_size=3, padding=1)  \n",
    "- BatchNorm2d(256)  \n",
    "- SiLU  \n",
    "- Dropout2d(0.20)  \n",
    "- Residual (1√ó1 conv)  \n",
    "- SiLU  \n",
    "- MaxPool2d(2,2)  \n",
    "üìè **Salida:** `[256 √ó 28 √ó 28]`\n",
    "\n",
    "---\n",
    "\n",
    "#### üß± Bloque 4 ‚Äî ResidualBlock(256 ‚Üí 256)\n",
    "- Conv2d(256, 256, kernel_size=3, padding=1)  \n",
    "- BatchNorm2d(256)  \n",
    "- SiLU  \n",
    "- Conv2d(256, 256, kernel_size=3, padding=1)  \n",
    "- BatchNorm2d(256)  \n",
    "- SiLU  \n",
    "- Dropout2d(0.20)  \n",
    "- Residual (sin proyecci√≥n)  \n",
    "- SiLU  \n",
    "- MaxPool2d(2,2)  \n",
    "üìè **Salida:** `[256 √ó 14 √ó 14]`\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÑ Global Average Pooling\n",
    "- AdaptiveAvgPool2d((1,1))  \n",
    "üìè **Salida:** `[256 √ó 1 √ó 1]`\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† Clasificador (Fully Connected)\n",
    "- Flatten() ‚Üí `[256]`  \n",
    "- Linear(256 ‚Üí 128)  \n",
    "- SiLU  \n",
    "- Dropout(0.50)  \n",
    "- Linear(128 ‚Üí 2)  \n",
    "\n",
    "üì§ **Salida final:** `[2]`  \n",
    "*(Probabilidades: Benign / Malignant)*\n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ Interpretaci√≥n general\n",
    "\n",
    "| Etapa | Tipo de capa | Tama√±o de salida | Descripci√≥n |\n",
    "|:------|:--------------|:----------------:|:-------------|\n",
    "| Entrada | Imagen RGB | [3√ó224√ó224] | Imagen normalizada de entrada |\n",
    "| Bloque 1 | Residual conv | [64√ó112√ó112] | Detecta bordes, colores y texturas b√°sicas |\n",
    "| Bloque 2 | Residual conv | [128√ó56√ó56] | Aprende formas intermedias y estructuras |\n",
    "| Bloque 3 | Residual conv | [256√ó28√ó28] | Capta regiones m√°s amplias y patrones complejos |\n",
    "| Bloque 4 | Residual conv | [256√ó14√ó14] | Refina las caracter√≠sticas m√°s profundas |\n",
    "| GAP | Global Avg Pool | [256√ó1√ó1] | Resume cada mapa de activaci√≥n a un valor medio |\n",
    "| FC1 | Densa | [128] | Combina caracter√≠sticas extra√≠das |\n",
    "| FC2 | Densa (salida) | [2] | Clasifica en Benigno / Maligno |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Resumen t√©cnico\n",
    "\n",
    "- üîπ **Capas convolucionales totales:** 8 (2 por bloque √ó 4 bloques)  \n",
    "- üîπ **Capas residuales:** 4 (cada una con posible proyecci√≥n 1√ó1)  \n",
    "- üîπ **Capas densas:** 2  \n",
    "- üîπ **Funciones de activaci√≥n:** `SiLU` (suave y estable, ideal para evitar saturaci√≥n)  \n",
    "- üîπ **Regularizaci√≥n:** `Dropout(0.20)` en convs y `Dropout(0.50)` en FC  \n",
    "- üîπ **Tama√±o de entrada:** 224√ó224 p√≠xeles  \n",
    "- üîπ **Tama√±o de salida:** 2 neuronas (clasificaci√≥n binaria)  \n",
    "- üîπ **Par√°metros entrenables aprox.:** entre **3,5 y 4 millones**\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Resumen conceptual\n",
    "\n",
    "Este modelo combina:\n",
    "- **Bloques residuales profundos**, que facilitan el flujo del gradiente y reducen el sobreajuste.  \n",
    "- **Capas convolucionales dobles**, que permiten extraer rasgos jer√°rquicos complejos.  \n",
    "- **Pooling progresivo**, que reduce la dimensionalidad manteniendo la informaci√≥n esencial.  \n",
    "- **Un clasificador totalmente conectado**, que interpreta las representaciones y decide si la lesi√≥n es benigna o maligna.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ En resumen\n",
    "\n",
    "Tu `SmallCNN_Res` es una CNN **profunda, eficiente y con buena capacidad de generalizaci√≥n**,  \n",
    "capaz de analizar patrones complejos en im√°genes dermatosc√≥picas y **distinguir entre melanomas malignos y lesiones benignas**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195734c3",
   "metadata": {},
   "source": [
    "## üß© Celda 1 ‚Äì Configuraci√≥n inicial\n",
    "\n",
    "Define todas las **importaciones, rutas y par√°metros globales** del proyecto:\n",
    "- Establece las carpetas de trabajo (`train`, `val`, `test`).\n",
    "- Fija las **semillas aleatorias** para asegurar reproducibilidad.\n",
    "- Configura el **dispositivo de entrenamiento** (en este caso, CPU).\n",
    "- Define hiperpar√°metros como `BATCH`, `EPOCHS`, `LR` y el tama√±o de imagen.\n",
    "\n",
    "> üí° Esta celda no genera resultados visibles, pero es clave para mantener consistencia y evitar comportamientos aleatorios en el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5748101a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\proyectos\\Caso_aprendizaje-Melanomas\\data\\raw\\train exists: True\n",
      "D:\\proyectos\\Caso_aprendizaje-Melanomas\\data\\raw\\val exists: True\n",
      "D:\\proyectos\\Caso_aprendizaje-Melanomas\\data\\raw\\test exists: True\n",
      "Device for training: cpu\n",
      "CPU threads: 7\n"
     ]
    }
   ],
   "source": [
    "# === Celda 1: Imports, rutas, device y par√°metros (CPU) ===\n",
    "from pathlib import Path\n",
    "import os, time, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# ---- Rutas (aj√∫stalas si hace falta) ----\n",
    "DATA_DIR = Path(r\"D:/proyectos/Caso_aprendizaje-Melanomas/data/raw\")\n",
    "TRAIN_DIR = DATA_DIR / \"train\"\n",
    "VAL_DIR   = DATA_DIR / \"val\"\n",
    "TEST_DIR  = DATA_DIR / \"test\"\n",
    "\n",
    "for p in (TRAIN_DIR, VAL_DIR, TEST_DIR):\n",
    "    print(p, \"exists:\", p.exists())\n",
    "\n",
    "# ---- Semillas fijas ----\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# ---- Device (solo CPU) + optimizaciones para CPU ----\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Device for training:\", device)\n",
    "\n",
    "# Usa la mayor√≠a de n√∫cleos para acelerar en CPU\n",
    "num_threads = max(1, (os.cpu_count() or 4) - 1)\n",
    "torch.set_num_threads(num_threads)\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")  # mejora BLAS en CPU modernas\n",
    "except Exception:\n",
    "    pass\n",
    "print(f\"CPU threads: {num_threads}\")\n",
    "\n",
    "# Desactiva flags de CUDA (no aplican en CPU)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ---- Par√°metros de datos/entreno (pensados para CPU) ----\n",
    "CLASSES = 2\n",
    "ROWS = COLS = 224        # puedes subir a 256 si te cabe en RAM y va fluido\n",
    "BATCH = 16               # en CPU suele aguantar m√°s batch que tu GPU de 4GB\n",
    "EPOCHS = 20              # m√°s √©pocas para apretar el fine-tune\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_WORKERS = 4          # en CPU paraleliza carga de datos (prueba 4‚Äì8 seg√∫n equipo)\n",
    "PIN_MEMORY = False       # en CPU no hace falta\n",
    "USE_AMP = False          # AMP solo aporta en GPU\n",
    "\n",
    "# ---- Normalizaci√≥n (ImageNet) ----\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a175753",
   "metadata": {},
   "source": [
    "## üß† Celda 2 ‚Äì Transformaciones y carga de datos\n",
    "\n",
    "Aplica un **tratamiento y aumento de datos (data augmentation)** moderado para el conjunto de entrenamiento:\n",
    "- Rotaciones, volteos, recortes aleatorios, ajustes de brillo y contraste.\n",
    "- Normalizaci√≥n seg√∫n los valores de *ImageNet*.\n",
    "- Balancea las clases usando un **WeightedRandomSampler**, para evitar que la clase \"Benign\" domine.\n",
    "\n",
    "> üìä El objetivo es que el modelo vea im√°genes variadas de las lesiones y aprenda a reconocer patrones robustos, evitando el sobreajuste a casos muy concretos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "262abd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases detectadas: ['Benign', 'Malignant']\n",
      "Class counts: [5346 4752] -> weights: [0.0002 0.0002]\n",
      "Clases (dataset real): ['Benign', 'Malignant']\n",
      "‚öôÔ∏è Precaching TRAIN (primera vez)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precacheando train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10098/10098 [09:45<00:00, 17.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Precaching VAL (primera vez)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precacheando val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1781/1781 [01:45<00:00, 16.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Features listas en memoria.\n",
      "‚úÖ DataLoaders listos.\n",
      "   Train: 10098 imgs | Val: 1781 imgs\n"
     ]
    }
   ],
   "source": [
    "# === Celda 2: Features + Dataset con features + Transforms + Pesos + Cach√© + DataLoaders ===\n",
    "# Requisitos previos en Celda 1:\n",
    "# DATA_DIR, TRAIN_DIR, VAL_DIR, CLASSES, ROWS, COLS, BATCH, NUM_WORKERS, PIN_MEMORY, IMAGENET_MEAN, IMAGENET_STD\n",
    "\n",
    "# ------------------ Imports base ------------------\n",
    "import os, json, time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- libs para features ---\n",
    "import cv2\n",
    "from skimage import morphology, measure, color, filters\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 2.5  EXTRACCI√ìN DE FEATURES (segmentaci√≥n + morfolog√≠a + color)\n",
    "# ======================================================================\n",
    "def largest_component(mask: np.ndarray) -> np.ndarray:\n",
    "    labeled = measure.label(mask, connectivity=2)\n",
    "    if labeled.max() == 0:\n",
    "        return mask\n",
    "    counts = np.bincount(labeled.ravel())\n",
    "    counts[0] = 0\n",
    "    keep = counts.argmax()\n",
    "    return (labeled == keep).astype(np.uint8)\n",
    "\n",
    "def lesion_mask_binarize(img_bgr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Segmentaci√≥n r√°pida basada en V (HSV) + Otsu + morfolog√≠a.\"\"\"\n",
    "    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n",
    "    v   = hsv[...,2]\n",
    "    v_blur = cv2.GaussianBlur(v, (5,5), 0)\n",
    "    thr = filters.threshold_otsu(v_blur)\n",
    "    mask = (v_blur < thr).astype(np.uint8)  # lesi√≥n suele ser m√°s oscura\n",
    "\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE,\n",
    "                            cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7,7)), iterations=2)\n",
    "    mask = morphology.remove_small_objects(mask.astype(bool), min_size=200).astype(np.uint8)\n",
    "    mask = morphology.remove_small_holes(mask.astype(bool), area_threshold=200).astype(np.uint8)\n",
    "    mask = largest_component(mask)\n",
    "    return mask\n",
    "\n",
    "def symmetry_scores(mask: np.ndarray) -> tuple[float,float]:\n",
    "    \"\"\"Asimetr√≠a izquierda-derecha y arriba-abajo (IoU; 1=sim√©trico).\"\"\"\n",
    "    h, w = mask.shape\n",
    "    left  = mask[:, :w//2]\n",
    "    right = np.fliplr(mask[:, w - w//2:])\n",
    "    top   = mask[:h//2, :]\n",
    "    bot   = np.flipud(mask[h - h//2:, :])\n",
    "\n",
    "    def iou(a,b):\n",
    "        inter = np.logical_and(a,b).sum()\n",
    "        union = np.logical_or(a,b).sum()\n",
    "        return inter/union if union>0 else 0.0\n",
    "\n",
    "    return iou(left, right), iou(top, bot)\n",
    "\n",
    "def color_features(img_bgr: np.ndarray, mask: np.ndarray, k_colors: int = 3) -> dict:\n",
    "    \"\"\"Medias/desv de HSV en la m√°scara + diversidad de color (KMeans en S,V).\"\"\"\n",
    "    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n",
    "    m = mask.astype(bool)\n",
    "    if m.sum() == 0:\n",
    "        return {k:0.0 for k in [\"h_mean\",\"h_std\",\"s_mean\",\"s_std\",\"v_mean\",\"v_std\",\"color_entropy\",\"kspread\"]}\n",
    "\n",
    "    H = hsv[...,0][m].astype(np.float32)         # 0..180 en OpenCV\n",
    "    S = hsv[...,1][m].astype(np.float32)/255.0\n",
    "    V = hsv[...,2][m].astype(np.float32)/255.0\n",
    "\n",
    "    feats = {\n",
    "        \"h_mean\": float(H.mean()), \"h_std\": float(H.std()),\n",
    "        \"s_mean\": float(S.mean()), \"s_std\": float(S.std()),\n",
    "        \"v_mean\": float(V.mean()), \"v_std\": float(V.std()),\n",
    "    }\n",
    "\n",
    "    sv = np.stack([S, V], axis=1)\n",
    "    k = min(k_colors, len(sv))\n",
    "    if k >= 2:\n",
    "        km = KMeans(n_clusters=k, n_init=5, random_state=42)\n",
    "        labels = km.fit_predict(sv)\n",
    "        counts = np.bincount(labels, minlength=k).astype(np.float32)\n",
    "        p = counts / counts.sum()\n",
    "        entropy = -np.sum(p * np.log(p + 1e-12))\n",
    "        centers = km.cluster_centers_\n",
    "        kspread = float(np.linalg.norm(centers.max(0) - centers.min(0)))\n",
    "        feats[\"color_entropy\"] = float(entropy)\n",
    "        feats[\"kspread\"] = kspread\n",
    "    else:\n",
    "        feats[\"color_entropy\"] = 0.0\n",
    "        feats[\"kspread\"] = 0.0\n",
    "\n",
    "    return feats\n",
    "\n",
    "def shape_features(mask: np.ndarray) -> dict:\n",
    "    \"\"\"√Årea relativa, per√≠metro relativo, circularidad, irregularidad, excentricidad.\"\"\"\n",
    "    m = mask.astype(bool)\n",
    "    area = m.sum()\n",
    "    if area == 0:\n",
    "        return {k:0.0 for k in [\n",
    "            \"area_rel\",\"perim_rel\",\"circularity\",\"irregularity\",\"eccentricity\",\"sym_lr\",\"sym_tb\"\n",
    "        ]}\n",
    "    h, w = m.shape\n",
    "\n",
    "    # ‚úÖ skimage ‚â•0.20 usa 'neighborhood'; algunas versiones antiguas aceptaban 'neighbourhood'\n",
    "    try:\n",
    "        perim = measure.perimeter(m, neighborhood=8)\n",
    "    except TypeError:\n",
    "        # Fallback para versiones antiguas (por si acaso)\n",
    "        perim = measure.perimeter(m, neighbourhood=8)\n",
    "\n",
    "    area_rel  = area / (h*w)\n",
    "    perim_rel = perim / (h+w)\n",
    "    circularity = (4*np.pi*area) / (perim**2 + 1e-12)\n",
    "    irregularity = (perim**2) / (4*np.pi*area + 1e-12)\n",
    "\n",
    "    props = measure.regionprops(m.astype(np.uint8))[0]\n",
    "    eccentricity = float(props.eccentricity)\n",
    "\n",
    "    sym_lr, sym_tb = symmetry_scores(mask)\n",
    "    return {\n",
    "        \"area_rel\": float(area_rel),\n",
    "        \"perim_rel\": float(perim_rel),\n",
    "        \"circularity\": float(circularity),\n",
    "        \"irregularity\": float(irregularity),\n",
    "        \"eccentricity\": float(eccentricity),\n",
    "        \"sym_lr\": float(sym_lr),\n",
    "        \"sym_tb\": float(sym_tb),\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_features_from_path(img_path: str) -> tuple[np.ndarray, dict, np.ndarray]:\n",
    "    \"\"\"Devuelve (img_bgr, dict_features, mask).\"\"\"\n",
    "    img_bgr = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
    "    if img_bgr is None:\n",
    "        raise FileNotFoundError(img_path)\n",
    "    mask = lesion_mask_binarize(img_bgr)\n",
    "    f_shape = shape_features(mask)\n",
    "    f_color = color_features(img_bgr, mask, k_colors=3)\n",
    "    feats = {**f_shape, **f_color}\n",
    "    return img_bgr, feats, mask\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 2.6  DATASET que a√±ade vector de features al sample\n",
    "# ======================================================================\n",
    "FEATURE_KEYS = [\n",
    "    \"area_rel\",\"perim_rel\",\"circularity\",\"irregularity\",\"eccentricity\",\n",
    "    \"sym_lr\",\"sym_tb\",\"h_mean\",\"h_std\",\"s_mean\",\"s_std\",\"v_mean\",\"v_std\",\n",
    "    \"color_entropy\",\"kspread\"\n",
    "]\n",
    "\n",
    "class ImageFolderWithFeatures(ImageFolder):\n",
    "    def __init__(self, root, transform=None, cache=True):\n",
    "        super().__init__(root, transform=transform)\n",
    "        self.cache = cache\n",
    "        self._cache = {}\n",
    "\n",
    "    def _get_feats(self, path):\n",
    "        if self.cache and path in self._cache:\n",
    "            return self._cache[path]\n",
    "        _, feats, _ = extract_features_from_path(path)\n",
    "        x = np.array([feats[k] for k in FEATURE_KEYS], dtype=np.float32)\n",
    "        if self.cache:\n",
    "            self._cache[path] = x\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, target = self.samples[idx]\n",
    "        sample = self.loader(path)  # PIL\n",
    "        img = self.transform(sample) if self.transform is not None else TF.to_tensor(sample)\n",
    "        feats = torch.from_numpy(self._get_feats(path))  # [F]\n",
    "        return img, target, feats\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 2.0  TRANSFORMS + pesos por clase (antes de DataLoaders)\n",
    "# ======================================================================\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((ROWS, COLS), scale=(0.70, 1.00), ratio=(0.90, 1.10)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15, fill=(128, 128, 128)),\n",
    "    transforms.ColorJitter(brightness=0.20, contrast=0.20, saturation=0.10, hue=0.02),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "    transforms.RandomErasing(p=0.25, scale=(0.02, 0.08), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize((ROWS, COLS)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Descubrir clases/pesos\n",
    "_tmp_ds = datasets.ImageFolder(TRAIN_DIR)\n",
    "print(\"Clases detectadas:\", _tmp_ds.classes)\n",
    "assert len(_tmp_ds.classes) == CLASSES, f\"Esperaba {CLASSES} clases, encontr√© {len(_tmp_ds.classes)}.\"\n",
    "targets_for_weights = np.array([y for _, y in _tmp_ds.samples])\n",
    "CLASS_COUNTS_NP  = np.bincount(targets_for_weights, minlength=CLASSES)\n",
    "CLASS_WEIGHTS_NP = 1.0 / (CLASS_COUNTS_NP + 1e-6)\n",
    "CLASSES_LIST     = _tmp_ds.classes\n",
    "print(\"Class counts:\", CLASS_COUNTS_NP, \"-> weights:\", np.round(CLASS_WEIGHTS_NP, 4))\n",
    "del _tmp_ds\n",
    "\n",
    "# ======================================================================\n",
    "# 2.3  Crear DATASETS con features\n",
    "# ======================================================================\n",
    "train_ds = ImageFolderWithFeatures(TRAIN_DIR, transform=train_tfms, cache=True)\n",
    "val_ds   = ImageFolderWithFeatures(VAL_DIR,   transform=val_tfms,   cache=True)\n",
    "print(\"Clases (dataset real):\", train_ds.classes)\n",
    "\n",
    "# ======================================================================\n",
    "# 2.7  CACH√â de features (cargar si existe; si no, precalcular y guardar)\n",
    "# ======================================================================\n",
    "CACHE_DIR = DATA_DIR / \"_feats_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def precache_features(ds):\n",
    "    for path, _ in tqdm(ds.samples, total=len(ds.samples), desc=f\"Precacheando {os.path.basename(ds.root)}\"):\n",
    "        _ = ds._get_feats(path)\n",
    "\n",
    "def save_cache(ds, out_json):\n",
    "    serial = {p: ds._cache[p].tolist() for p,_ in ds.samples if p in ds._cache}\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(serial, f)\n",
    "\n",
    "def load_cache(ds, in_json):\n",
    "    if not os.path.exists(in_json):\n",
    "        return False\n",
    "    with open(in_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    ds._cache.update({k: np.array(v, dtype=np.float32) for k, v in data.items()})\n",
    "    print(f\"Cache cargada desde: {in_json} ({len(data)} entradas)\")\n",
    "    return True\n",
    "\n",
    "loaded_train = load_cache(train_ds, str(CACHE_DIR / \"train_feats.json\"))\n",
    "loaded_val   = load_cache(val_ds,   str(CACHE_DIR / \"val_feats.json\"))\n",
    "\n",
    "if not loaded_train:\n",
    "    print(\"‚öôÔ∏è Precaching TRAIN (primera vez)...\")\n",
    "    precache_features(train_ds)\n",
    "    save_cache(train_ds, str(CACHE_DIR / \"train_feats.json\"))\n",
    "\n",
    "if not loaded_val:\n",
    "    print(\"‚öôÔ∏è Precaching VAL (primera vez)...\")\n",
    "    precache_features(val_ds)\n",
    "    save_cache(val_ds, str(CACHE_DIR / \"val_feats.json\"))\n",
    "\n",
    "print(\"‚úÖ Features listas en memoria.\")\n",
    "\n",
    "# ======================================================================\n",
    "# 2.8  DATALOADERS balanceados (con collate que incluye features)\n",
    "# ======================================================================\n",
    "targets = np.array([y for _, y in train_ds.samples])\n",
    "sample_weights = CLASS_WEIGHTS_NP[targets]\n",
    "sampler = WeightedRandomSampler(weights=sample_weights,\n",
    "                                num_samples=len(sample_weights),\n",
    "                                replacement=True)\n",
    "\n",
    "def collate_with_feats(batch):\n",
    "    imgs = torch.stack([b[0] for b in batch], dim=0)\n",
    "    ys   = torch.tensor([b[1] for b in batch], dtype=torch.long)\n",
    "    fs   = torch.stack([b[2] for b in batch], dim=0)\n",
    "    return imgs, ys, fs\n",
    "\n",
    "persistent = True if NUM_WORKERS and NUM_WORKERS > 0 else False\n",
    "\n",
    "train_dl = DataLoader(train_ds,\n",
    "                      batch_size=BATCH,\n",
    "                      sampler=sampler,\n",
    "                      num_workers=NUM_WORKERS,\n",
    "                      pin_memory=PIN_MEMORY,\n",
    "                      persistent_workers=persistent,\n",
    "                      collate_fn=collate_with_feats)\n",
    "\n",
    "val_dl = DataLoader(val_ds,\n",
    "                    batch_size=BATCH,\n",
    "                    shuffle=False,\n",
    "                    num_workers=NUM_WORKERS,\n",
    "                    pin_memory=PIN_MEMORY,\n",
    "                    persistent_workers=persistent,\n",
    "                    collate_fn=collate_with_feats)\n",
    "\n",
    "print(\"‚úÖ DataLoaders listos.\")\n",
    "print(f\"   Train: {len(train_ds)} imgs | Val: {len(val_ds)} imgs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f01980a",
   "metadata": {},
   "source": [
    "## üî¨ Celda 3 ‚Äì Definici√≥n del modelo convolucional mejorado\n",
    "\n",
    "Define la arquitectura `SmallCNN_Res_Fusion`, basada en **bloques residuales**:\n",
    "- 4 bloques de convoluciones con conexiones *skip* para estabilidad.\n",
    "- Funci√≥n de activaci√≥n *SiLU* (m√°s suave que ReLU).\n",
    "- Promedio global (GAP) + capa totalmente conectada con fusi√≥n de features.\n",
    "\n",
    "> üí™ Este modelo es m√°s profundo y estable que la versi√≥n simple.  \n",
    "> Gracias a las conexiones residuales, se entrena mejor incluso con conjuntos de datos limitados y capta texturas y estructuras m√°s complejas de la piel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6836a713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par√°metros entrenables (fusi√≥n): 2,404,098\n"
     ]
    }
   ],
   "source": [
    "# === Celda 3: Modelo CNN + FUSI√ìN de features (doble conv + residual) ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "# N¬∫ de features adicionales (definido por tu Celda 2.6)\n",
    "N_FEATS = len(FEATURE_KEYS)  # p.ej., 15\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Conv3x3 -> BN -> SiLU -> Conv3x3 -> BN -> SiLU -> Dropout2d\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, drop=0.20):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.SiLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.SiLU(inplace=True),\n",
    "\n",
    "            nn.Dropout2d(drop),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    x ‚îÄ‚îÄ‚ñ∫ DoubleConv ‚îÄ‚îÄ‚ñ∫ + (proyecci√≥n 1x1 si cambia canal) ‚îÄ‚îÄ‚ñ∫ SiLU ‚îÄ‚îÄ‚ñ∫ MaxPool(2)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, drop=0.20):\n",
    "        super().__init__()\n",
    "        self.double = DoubleConv(in_ch, out_ch, drop=drop)\n",
    "        self.proj: Optional[nn.Module] = None\n",
    "        if in_ch != out_ch:\n",
    "            self.proj = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "        self.act  = nn.SiLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.double(x)\n",
    "        skip = x if self.proj is None else self.proj(x)\n",
    "        y = self.act(y + skip)\n",
    "        y = self.pool(y)\n",
    "        return y\n",
    "\n",
    "class SmallCNN_Res_Fusion(nn.Module):\n",
    "    \"\"\"\n",
    "    4 bloques residuales (8 convs): 224‚Üí112‚Üí56‚Üí28‚Üí14\n",
    "    GAP -> concat([feat_map, features_extras]) -> MLP -> logits\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=3, num_classes=2, drop_conv=0.20, drop_fc=0.50, n_feats=N_FEATS):\n",
    "        super().__init__()\n",
    "        self.block1 = ResidualBlock(in_ch,   64,  drop=drop_conv)\n",
    "        self.block2 = ResidualBlock(64,      128, drop=drop_conv)\n",
    "        self.block3 = ResidualBlock(128,     256, drop=drop_conv)\n",
    "        self.block4 = ResidualBlock(256,     256, drop=drop_conv)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1,1))  # [B,256,1,1]\n",
    "        self.flatten = nn.Flatten()             # -> [B,256]\n",
    "        self.n_feats = n_feats\n",
    "\n",
    "        # Cabeza de fusi√≥n: concatena [256] con [n_feats]\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(256 + n_feats, 128),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Dropout(drop_fc),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "        # Inicializaci√≥n Kaiming para estabilidad\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x, feats):\n",
    "        # x: [B,3,H,W], feats: [B, n_feats]\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.flatten(x)                 # [B,256]\n",
    "        # Asegura tipo/shape de feats\n",
    "        if feats.dim() == 1:\n",
    "            feats = feats.unsqueeze(0)\n",
    "        feats = feats.to(x.dtype)\n",
    "        z = torch.cat([x, feats], dim=1)    # [B, 256+n_feats]\n",
    "        out = self.head(z)                  # [B,num_classes]\n",
    "        return out\n",
    "\n",
    "# Instanciaci√≥n\n",
    "model = SmallCNN_Res_Fusion(in_ch=3, num_classes=CLASSES,\n",
    "                            drop_conv=0.20, drop_fc=0.50, n_feats=N_FEATS).to(device)\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Par√°metros entrenables (fusi√≥n): {params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4df2b",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Celda 4 ‚Äì Definici√≥n de la funci√≥n de p√©rdida y optimizador\n",
    "\n",
    "- Usa **CrossEntropyLoss** con pesos por clase para corregir el desequilibrio entre \"Benign\" y \"Malignant\".\n",
    "- Emplea el optimizador **AdamW**, que combina Adam con regularizaci√≥n L2 (controla el sobreajuste).\n",
    "- Define la m√©trica de precisi√≥n (`accuracy_from_logits`).\n",
    "\n",
    "> ‚öñÔ∏è Esta celda ajusta c√≥mo el modelo aprende (descenso del gradiente) y penaliza los errores de forma equilibrada entre las dos clases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3b7e68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: AdamW | LR = 0.0003 | weight_decay = 0.0001\n"
     ]
    }
   ],
   "source": [
    "# === Celda 4: Loss, optimizador y m√©trica (CPU) ===\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Toma los pesos de clase desde la Celda 2 (fallback si cambiaste el nombre)\n",
    "_weights_np = globals().get(\"CLASS_WEIGHTS_NP\", globals().get(\"class_weights\", None))\n",
    "assert _weights_np is not None, \"No encuentro CLASS_WEIGHTS_NP ni class_weights (revisa Celda 2).\"\n",
    "\n",
    "w = torch.tensor(_weights_np, dtype=torch.float32, device=device)\n",
    "\n",
    "# P√©rdida ponderada + label smoothing suave\n",
    "criterion = nn.CrossEntropyLoss(weight=w, label_smoothing=0.05)\n",
    "\n",
    "# Optimizador con buen decay (mejor que Adam para fine-tune)\n",
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# M√©trica principal: accuracy\n",
    "def accuracy_from_logits(logits, targets):\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == targets).float().mean().item()\n",
    "\n",
    "print(\"Optimizer: AdamW | LR =\", LR, \"| weight_decay =\", WEIGHT_DECAY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd593a1",
   "metadata": {},
   "source": [
    "## üöÄ Celda 5 ‚Äì Entrenamiento y validaci√≥n\n",
    "\n",
    "Ejecuta el bucle de entrenamiento:\n",
    "1. **Entrena** el modelo sobre `train_dl` durante varias √©pocas.\n",
    "2. **Eval√∫a** sobre `val_dl` en cada √©poca.\n",
    "3. Usa **early stopping** si la p√©rdida de validaci√≥n deja de mejorar.\n",
    "4. Guarda el mejor modelo (`.pth`) basado en la p√©rdida m√≠nima.\n",
    "\n",
    "> üìà Aqu√≠ se observa si el modelo realmente aprende:  \n",
    "> - La *p√©rdida de entrenamiento* debe disminuir progresivamente.  \n",
    "> - La *p√©rdida de validaci√≥n* no debe aumentar mucho (evita overfitting).  \n",
    "> - Las *precisiones* de train y val deben estar pr√≥ximas si el modelo generaliza bien.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa59743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\proyectos\\Caso_aprendizaje-Melanomas\\.venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# === Celda 5: Entreno/validaci√≥n (CPU, sin AMP) ‚Äî compatible con features ===\n",
    "\n",
    "def _unpack_batch(batch):\n",
    "    # batch puede ser (x,y,f) o (x,y)\n",
    "    if isinstance(batch, (list, tuple)) and len(batch) == 3:\n",
    "        x, y, f = batch\n",
    "    else:\n",
    "        x, y = batch\n",
    "        f = None\n",
    "    return x, y, f\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, clip_grad_norm=1.0, log_every=20):\n",
    "    \"\"\"\n",
    "    Entrena una √©poca completa y muestra progreso por batches.\n",
    "    log_every: muestra info cada N batches.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = total_acc = n = 0\n",
    "    n_batches = len(loader)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, batch in enumerate(loader):\n",
    "        x, y, f = _unpack_batch(batch)\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        if f is not None: f = f.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x, f) if f is not None else model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        # (opcional) clipping para estabilidad en CPU\n",
    "        if clip_grad_norm and clip_grad_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc  += accuracy_from_logits(logits, y)\n",
    "        n += 1\n",
    "\n",
    "        # üîπ Mostrar progreso cada log_every batches\n",
    "        if (i + 1) % log_every == 0 or (i + 1) == n_batches:\n",
    "            pct = 100 * (i + 1) / n_batches\n",
    "            avg_loss = total_loss / n\n",
    "            avg_acc  = total_acc / n\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"[Batch {i+1:4d}/{n_batches}] {pct:5.1f}% | Loss={avg_loss:.4f} | \"\n",
    "                  f\"Acc={avg_acc:.4f} | Tiempo={elapsed:.1f}s\")\n",
    "\n",
    "    epoch_loss = total_loss / max(n, 1)\n",
    "    epoch_acc  = total_acc / max(n, 1)\n",
    "    print(f\"‚úÖ √âpoca completada: loss={epoch_loss:.4f}, acc={epoch_acc:.4f}, \"\n",
    "          f\"duraci√≥n total={time.time()-start_time:.1f}s\\n\")\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = total_acc = n = 0\n",
    "    for batch in loader:\n",
    "        x, y, f = _unpack_batch(batch)\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        if f is not None: f = f.to(device)\n",
    "\n",
    "        logits = model(x, f) if f is not None else model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc  += accuracy_from_logits(logits, y)\n",
    "        n += 1\n",
    "    return total_loss / max(n, 1), total_acc / max(n, 1)\n",
    "\n",
    "history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
    "BEST_PATH = \"smallcnn_res_fusion_cpu_best.pth\"  # nombre coherente con el modelo fusionado\n",
    "\n",
    "# === Scheduler + Early Stopping (usa el optimizer de Celda 4) ===\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "EARLY_PATIENCE = 6\n",
    "no_improve = 0\n",
    "best_val = float('inf')\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_dl, optimizer, criterion, clip_grad_norm=1.0)\n",
    "    va_loss, va_acc = validate(model, val_dl, criterion)\n",
    "\n",
    "    # guarda historial para las curvas\n",
    "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "    # step del scheduler en funci√≥n de val_loss\n",
    "    scheduler.step(va_loss)\n",
    "\n",
    "    # guarda mejor checkpoint por val_loss\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"classes\": CLASSES_LIST if 'CLASSES_LIST' in globals() else None,\n",
    "            \"img_size\": (ROWS, COLS),\n",
    "            \"feature_keys\": FEATURE_KEYS  # para asegurar compatibilidad al cargar\n",
    "        }, BEST_PATH)\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= EARLY_PATIENCE:\n",
    "            print(\"Early stopping üö¶\")\n",
    "            break\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    cur_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(f\"[{epoch:02d}] train_loss={tr_loss:.4f} acc={tr_acc:.4f} | \"\n",
    "          f\"val_loss={va_loss:.4f} acc={va_acc:.4f} | lr={cur_lr:.2e} | {dt:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba074fdd",
   "metadata": {},
   "source": [
    "## üßÆ Celda 6 ‚Äì B√∫squeda del umbral √≥ptimo\n",
    "\n",
    "Explora diferentes **umbrales de decisi√≥n** para clasificar una imagen como ‚ÄúMalignant‚Äù.\n",
    "- Calcula F1-score, Recall y Precision para cada umbral.\n",
    "- Escoge el valor que **maximiza el F1-score**, equilibrio entre precisi√≥n y sensibilidad.\n",
    "\n",
    "> üîç Este paso mejora la clasificaci√≥n en problemas m√©dicos, donde el objetivo es reducir **falsos negativos** (no pasar por alto melanomas malignos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7345c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Celda 6: Buscar UMBRAL √ìPTIMO en validaci√≥n (compatible con features) ===\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def _unpack_batch_for_probs(batch):\n",
    "    # batch puede ser (x,y,f) o (x,y)\n",
    "    if isinstance(batch, (list, tuple)) and len(batch) == 3:\n",
    "        x, y, f = batch\n",
    "    else:\n",
    "        x, y = batch\n",
    "        f = None\n",
    "    return x, y, f\n",
    "\n",
    "def collect_probs_labels(model, loader, malignant_idx=1):\n",
    "    \"\"\"Devuelve prob(Malignant) y etiquetas verdaderas desde un loader.\"\"\"\n",
    "    model.eval()\n",
    "    probs, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x, y, f = _unpack_batch_for_probs(batch)\n",
    "            x = x.to(device)\n",
    "            if f is not None:\n",
    "                f = f.to(device)\n",
    "                logits = model(x, f)\n",
    "            else:\n",
    "                logits = model(x)\n",
    "            p = torch.softmax(logits, dim=1)[:, malignant_idx].cpu().numpy()\n",
    "            probs.append(p); labels.append(y.numpy())\n",
    "    return np.concatenate(probs), np.concatenate(labels)\n",
    "\n",
    "# 1) Probabilidades en validaci√≥n para la clase \"Malignant\" (√≠ndice 1)\n",
    "val_probs, val_y = collect_probs_labels(model, val_dl, malignant_idx=1)\n",
    "\n",
    "# 2) Barrido de umbrales\n",
    "ths = np.linspace(0.05, 0.95, 37)\n",
    "f1s, recs, precs = [], [], []\n",
    "best_f1, best_t_f1 = -1.0, 0.5\n",
    "best_rec, best_t_rec = -1.0, 0.5\n",
    "\n",
    "for t in ths:\n",
    "    preds = (val_probs >= t).astype(int)\n",
    "    f1  = f1_score(val_y, preds, pos_label=1, zero_division=0)\n",
    "    rec = recall_score(val_y, preds, pos_label=1, zero_division=0)\n",
    "    pre = precision_score(val_y, preds, pos_label=1, zero_division=0)\n",
    "    f1s.append(f1); recs.append(rec); precs.append(pre)\n",
    "    if f1 > best_f1: best_f1, best_t_f1 = f1, t\n",
    "    if rec > best_rec: best_rec, best_t_rec = rec, t\n",
    "\n",
    "print(f\"‚ñ™ Umbral √≥ptimo por F1: {best_t_f1:.2f} | F1={best_f1:.3f}\")\n",
    "print(f\"‚ñ™ Umbral que maximiza Recall: {best_t_rec:.2f} | Recall={best_rec:.3f}\")\n",
    "\n",
    "# 3) Elige el criterio principal\n",
    "OPT_THRESH = best_t_f1\n",
    "print(f\"OPT_THRESH = {OPT_THRESH:.2f} (usado por defecto)\")\n",
    "\n",
    "# 4) (Opcional) Visualiza F1/Recall/Precision vs Umbral\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(ths, f1s,  label=\"F1\")\n",
    "plt.plot(ths, recs, label=\"Recall\")\n",
    "plt.plot(ths, precs,label=\"Precision\")\n",
    "plt.axvline(OPT_THRESH, ls=\"--\")\n",
    "plt.xlabel(\"Umbral\"); plt.ylabel(\"M√©trica\")\n",
    "plt.title(\"M√©tricas en Validaci√≥n vs Umbral\")\n",
    "plt.grid(True, ls=\"--\", alpha=0.5); plt.legend()\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb4f28",
   "metadata": {},
   "source": [
    "## üßæ Celda 7 ‚Äì Evaluaci√≥n en el conjunto de test\n",
    "\n",
    "Eval√∫a el rendimiento del modelo final en las im√°genes **nunca vistas**:\n",
    "- Calcula la precisi√≥n total y por clase.  \n",
    "- Aplica el umbral √≥ptimo obtenido en validaci√≥n.  \n",
    "\n",
    "> üß™ Aqu√≠ se obtiene la medida real de rendimiento del modelo: c√≥mo se comportar√≠a con casos nuevos fuera del entrenamiento.\n",
    "> Si los resultados de *test* son similares a *val*, el modelo **generaliza correctamente**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff9561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Celda 7: Evaluaci√≥n en test (CPU, compatible con features) ===\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1) Cargar mejor checkpoint\n",
    "ckpt = torch.load(BEST_PATH, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "# 2) Dataset/Dataloader de test (usa tfms de validaci√≥n y la clase con features)\n",
    "test_ds = ImageFolderWithFeatures(TEST_DIR, transform=val_tfms, cache=True)\n",
    "persistent = True if NUM_WORKERS and NUM_WORKERS > 0 else False\n",
    "\n",
    "# collate igual que en train_dl/val_dl\n",
    "def collate_with_feats(batch):\n",
    "    imgs = torch.stack([b[0] for b in batch], dim=0)\n",
    "    ys   = torch.tensor([b[1] for b in batch], dtype=torch.long)\n",
    "    fs   = torch.stack([b[2] for b in batch], dim=0)\n",
    "    return imgs, ys, fs\n",
    "\n",
    "test_dl = DataLoader(test_ds,\n",
    "                     batch_size=BATCH,\n",
    "                     shuffle=False,\n",
    "                     num_workers=NUM_WORKERS,\n",
    "                     pin_memory=PIN_MEMORY,\n",
    "                     persistent_workers=persistent,\n",
    "                     collate_fn=collate_with_feats)\n",
    "\n",
    "# --- Funci√≥n auxiliar de desempaquetado ---\n",
    "def _unpack_batch(batch):\n",
    "    if isinstance(batch, (list, tuple)) and len(batch) == 3:\n",
    "        x, y, f = batch\n",
    "    else:\n",
    "        x, y = batch\n",
    "        f = None\n",
    "    return x, y, f\n",
    "\n",
    "# --- Evaluaci√≥n est√°ndar (argmax, sin umbral) ---\n",
    "@torch.no_grad()\n",
    "def evaluate_loader_argmax(model, loader):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    per_class = Counter(); per_class_correct = Counter()\n",
    "\n",
    "    for batch in loader:\n",
    "        x, y, f = _unpack_batch(batch)\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        if f is not None: f = f.to(device)\n",
    "\n",
    "        logits = model(x, f) if f is not None else model(x)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total   += y.numel()\n",
    "\n",
    "        for yi, pi in zip(y.tolist(), preds.tolist()):\n",
    "            per_class[yi] += 1\n",
    "            if yi == pi:\n",
    "                per_class_correct[yi] += 1\n",
    "\n",
    "    acc = correct / total if total else 0.0\n",
    "    return acc, per_class, per_class_correct\n",
    "\n",
    "# --- Evaluaci√≥n con umbral √≥ptimo (p(Malignant) >= thr) ---\n",
    "@torch.no_grad()\n",
    "def evaluate_loader_with_threshold(model, loader, malignant_idx=1, thr=0.5):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    for batch in loader:\n",
    "        x, y, f = _unpack_batch(batch)\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        if f is not None: f = f.to(device)\n",
    "\n",
    "        logits = model(x, f) if f is not None else model(x)\n",
    "        probs = torch.softmax(logits, dim=1)[:, malignant_idx]\n",
    "        preds = (probs >= thr).long()\n",
    "        correct += (preds == y).sum().item()\n",
    "        total   += y.numel()\n",
    "    return correct / total if total else 0.0\n",
    "\n",
    "# 3) Evaluaci√≥n y reporte por clase (argmax)\n",
    "test_acc, counts, rights = evaluate_loader_argmax(model, test_dl)\n",
    "print(f\"Test accuracy (argmax): {test_acc:.4f}\")\n",
    "idx2class = {i: c for i, c in enumerate(test_ds.classes)}\n",
    "for i in range(len(test_ds.classes)):\n",
    "    n = counts[i]; ok = rights[i]; name = idx2class[i]\n",
    "    if n > 0:\n",
    "        print(f\"  {name:>10s}: {ok}/{n} ({ok/n:.3f})\")\n",
    "    else:\n",
    "        print(f\"  {name:>10s}: 0/0 (N/A)\")\n",
    "\n",
    "# 4) (opcional) Evaluaci√≥n con umbral √≥ptimo\n",
    "if 'OPT_THRESH' in globals():\n",
    "    acc_thr = evaluate_loader_with_threshold(model, test_dl,\n",
    "                                             malignant_idx=1, thr=OPT_THRESH)\n",
    "    print(f\"\\nTest accuracy (umbral √≥ptimo={OPT_THRESH:.2f}): {acc_thr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec40733",
   "metadata": {},
   "source": [
    "## üìã Interpretaci√≥n del reporte con umbral √≥ptimo\n",
    "\n",
    "Esta celda eval√∫a el modelo final usando el **umbral de decisi√≥n ajustado (`OPT_THRESH`)**, obtenido en la celda anterior para maximizar el equilibrio entre *precisi√≥n* y *recall*.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Qu√© hace el c√≥digo\n",
    "1. **`collect_probs_labels`**: calcula las probabilidades de que cada imagen pertenezca a la clase *Malignant*.\n",
    "2. **`(test_probs >= OPT_THRESH)`**: convierte esas probabilidades en predicciones binarias (`0 = Benign`, `1 = Malignant`).\n",
    "3. **`classification_report`**: genera m√©tricas detalladas para cada clase.\n",
    "4. **`confusion_matrix`**: muestra cu√°ntas predicciones fueron correctas o incorrectas en cada categor√≠a.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä C√≥mo interpretar el reporte\n",
    "\n",
    "El `classification_report` devuelve cuatro m√©tricas principales para cada clase:\n",
    "\n",
    "| M√©trica | Significado | Interpretaci√≥n |\n",
    "|:---------|:-------------|:----------------|\n",
    "| **Precision** | De todos los casos que el modelo predijo como *Malignant*, cu√°ntos lo eran realmente. | Alta precisi√≥n = pocos falsos positivos. |\n",
    "| **Recall (Sensibilidad)** | De todos los *Malignant* reales, cu√°ntos detect√≥ el modelo. | Alta sensibilidad = pocos falsos negativos. |\n",
    "| **F1-score** | Media arm√≥nica entre precisi√≥n y recall. | Resume el equilibrio entre ambas m√©tricas. |\n",
    "| **Support** | N√∫mero real de ejemplos de esa clase en el conjunto de test. | Muestra el tama√±o de la muestra. |\n",
    "\n",
    "Adem√°s, el bloque final del reporte muestra:\n",
    "- **Accuracy global**: porcentaje total de aciertos del modelo.  \n",
    "- **Macro avg**: media simple entre ambas clases (sin ponderar).  \n",
    "- **Weighted avg**: media ponderada por la frecuencia de cada clase (m√°s realista si hay desequilibrio).\n",
    "\n",
    "---\n",
    "\n",
    "### üî≤ C√≥mo leer la matriz de confusi√≥n\n",
    "\n",
    "La matriz se organiza as√≠:\n",
    "\n",
    "|                 | Predicho Benign | Predicho Malignant |\n",
    "|:----------------|:----------------|:-------------------|\n",
    "| **Real Benign** | Verdaderos negativos (TN) | Falsos positivos (FP) |\n",
    "| **Real Malignant** | Falsos negativos (FN) | Verdaderos positivos (TP) |\n",
    "\n",
    "- **Diagonal principal (TN + TP)** ‚Üí casos correctamente clasificados.  \n",
    "- **Fuera de la diagonal (FP + FN)** ‚Üí errores del modelo.  \n",
    "\n",
    "> üí° En diagn√≥stico m√©dico, es especialmente importante **minimizar los falsos negativos (FN)**,  \n",
    "> ya que representan casos malignos que el modelo no detect√≥ correctamente.\n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ En resumen\n",
    "\n",
    "> Esta evaluaci√≥n muestra el **rendimiento real del modelo sobre im√°genes nuevas**, aplicando el umbral √≥ptimo encontrado.  \n",
    "> Un buen modelo debe mostrar:\n",
    "> - Alta *precision* y *recall* en la clase *Malignant*.\n",
    "> - Un *F1-score* equilibrado entre ambas clases.\n",
    "> - Una matriz de confusi√≥n con la mayor√≠a de los valores concentrados en la diagonal principal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluaci√≥n final con umbral √≥ptimo ===\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "test_probs, test_y = collect_probs_labels(model, test_dl, malignant_idx=1)\n",
    "test_pred = (test_probs >= OPT_THRESH).astype(int)\n",
    "\n",
    "print(\"\\nüìã Reporte con umbral √≥ptimo:\")\n",
    "print(classification_report(test_y, test_pred, target_names=test_ds.classes))\n",
    "print(\"Matriz de confusi√≥n (umbral ajustado):\\n\", confusion_matrix(test_y, test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca969c",
   "metadata": {},
   "source": [
    "## üé® Interpretaci√≥n de la matriz de confusi√≥n ajustada\n",
    "\n",
    "Esta celda **no calcula nuevas m√©tricas**, sino que **representa visualmente** la matriz de confusi√≥n obtenida en la evaluaci√≥n final.\n",
    "\n",
    "El objetivo es facilitar la **interpretaci√≥n visual de los aciertos y errores** del modelo:\n",
    "\n",
    "- El **color m√°s oscuro** en la diagonal indica una mayor cantidad de aciertos.  \n",
    "- Las **celdas fuera de la diagonal** representan errores:\n",
    "  - Falsos positivos (el modelo predijo *Malignant* pero era *Benign*).\n",
    "  - Falsos negativos (el modelo predijo *Benign* pero era *Malignant*).\n",
    "\n",
    "> üí° En diagn√≥stico m√©dico, es fundamental que la celda correspondiente a **Malignant ‚Üí Malignant** (parte inferior derecha) tenga un color intenso,  \n",
    "> ya que eso indica una **alta detecci√≥n de casos realmente malignos**.\n",
    "\n",
    "Adem√°s, esta versi√≥n:\n",
    "- Usa un **mapa de color azul (\"Blues\")** para mostrar las intensidades.\n",
    "- A√±ade **etiquetas num√©ricas** dentro de cada celda.\n",
    "- Guarda la imagen como `confusion_matrix_adjusted.png` para incluirla en informes o presentaciones.\n",
    "\n",
    "> En resumen, esta celda no cambia los resultados, pero **mejora la comprensi√≥n visual** de los aciertos y errores del modelo en el conjunto de test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d700a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Matriz de confusi√≥n AJUSTADA (grande y con colores) ===\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# --- Usa la versi√≥n compatible de collect_probs_labels (la que admite f) ---\n",
    "test_probs, test_y = collect_probs_labels(model, test_dl, malignant_idx=1)  # p(Malignant)\n",
    "test_pred = (test_probs >= OPT_THRESH).astype(int)\n",
    "\n",
    "# --- Matriz de confusi√≥n ---\n",
    "cm = confusion_matrix(test_y, test_pred)\n",
    "\n",
    "# --- Plot grande y con colorbar ---\n",
    "fig, ax = plt.subplots(figsize=(8.5, 7.5))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=test_ds.classes)\n",
    "disp.plot(cmap=\"Blues\", values_format='d', ax=ax, colorbar=True)\n",
    "\n",
    "ax.set_title(f\"Matriz de Confusi√≥n (Test) ‚Äî Umbral √≥ptimo = {OPT_THRESH:.2f}\",\n",
    "             fontsize=16, pad=12)\n",
    "ax.set_xlabel(\"Predicci√≥n\", fontsize=13)\n",
    "ax.set_ylabel(\"Etiqueta real\", fontsize=13)\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- (opcional) Guardar a disco ---\n",
    "fig, ax = plt.subplots(figsize=(8.5, 7.5))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=test_ds.classes)\n",
    "disp.plot(cmap=\"Blues\", values_format='d', ax=ax, colorbar=True)\n",
    "ax.set_title(f\"Matriz de Confusi√≥n (Test) ‚Äî Umbral √≥ptimo = {OPT_THRESH:.2f}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix_adjusted.png\", dpi=220, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"‚úÖ Guardada en: confusion_matrix_adjusted.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ff9306",
   "metadata": {},
   "source": [
    "## üìä Celda 7.5 ‚Äì Comparativa y an√°lisis de generalizaci√≥n\n",
    "\n",
    "Muestra un resumen global de m√©tricas (Loss, Accuracy, F1, Recall, Precision) para *train*, *val* y *test*.  \n",
    "Tambi√©n calcula los **gaps de generalizaci√≥n** entre entrenamiento y test.\n",
    "\n",
    "> üí¨ Interpretaci√≥n:\n",
    "> - Si el *gap* entre train y test es **< 2% ‚Üí excelente generalizaci√≥n**.  \n",
    "> - Entre 2‚Äì5% ‚Üí buena generalizaci√≥n.  \n",
    "> - > 5% ‚Üí *overfitting*.  \n",
    "> - Gap negativo grande ‚Üí *underfitting*.  \n",
    ">\n",
    "> El gr√°fico permite ver si el modelo mantiene un rendimiento estable fuera del conjunto de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6bb9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Celda 7.5: Evaluaci√≥n global y generalizaci√≥n del modelo ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "# --- M√©tricas finales de entrenamiento ---\n",
    "final_train_loss = history[\"train_loss\"][-1]\n",
    "final_val_loss   = history[\"val_loss\"][-1]\n",
    "final_train_acc  = history[\"train_acc\"][-1]\n",
    "final_val_acc    = history[\"val_acc\"][-1]\n",
    "\n",
    "# --- M√©tricas de test (usando OPT_THRESH) ---\n",
    "test_probs, test_y = collect_probs_labels(model, test_dl, malignant_idx=1)\n",
    "test_pred = (test_probs >= OPT_THRESH).astype(int)\n",
    "\n",
    "test_acc = np.mean(test_pred == test_y)\n",
    "test_f1  = f1_score(test_y, test_pred, pos_label=1)\n",
    "test_rec = recall_score(test_y, test_pred, pos_label=1)\n",
    "test_pre = precision_score(test_y, test_pred, pos_label=1)\n",
    "\n",
    "# --- Gaps de generalizaci√≥n (diferencias porcentuales) ---\n",
    "gap_train_val  = 100 * (final_train_acc - final_val_acc)\n",
    "gap_train_test = 100 * (final_train_acc - test_acc)\n",
    "\n",
    "# --- Tabla resumen ---\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Conjunto\": [\"Entrenamiento\", \"Validaci√≥n\", \"Test\"],\n",
    "    \"Loss\": [final_train_loss, final_val_loss, np.nan],\n",
    "    \"Accuracy\": [final_train_acc, final_val_acc, test_acc],\n",
    "    \"F1\": [np.nan, np.nan, test_f1],\n",
    "    \"Recall\": [np.nan, np.nan, test_rec],\n",
    "    \"Precision\": [np.nan, np.nan, test_pre]\n",
    "})\n",
    "\n",
    "display(metrics_df.round(4))\n",
    "\n",
    "# --- Gr√°fico de barras comparativo ---\n",
    "plt.style.use(\"seaborn-v0_8-muted\")\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sets = [\"Entrenamiento\", \"Validaci√≥n\", \"Test\"]\n",
    "accs = [final_train_acc, final_val_acc, test_acc]\n",
    "bars = ax.bar(sets, accs, color=[\"#57c057\", \"#c662c1\", \"#1f77b4\"], alpha=0.85)\n",
    "\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
    "ax.set_title(\"Comparativa de precisi√≥n final\", fontsize=14)\n",
    "ax.bar_label(bars, fmt=\"%.3f\", padding=3)\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- An√°lisis de generalizaci√≥n ---\n",
    "print(\"üìä Evaluaci√≥n de la generalizaci√≥n del modelo:\\n\")\n",
    "print(f\"‚ñ™ Diferencia Entrenamiento‚ÄìValidaci√≥n: {gap_train_val:+.2f}%\")\n",
    "print(f\"‚ñ™ Diferencia Entrenamiento‚ÄìTest:       {gap_train_test:+.2f}%\")\n",
    "\n",
    "if abs(gap_train_test) < 2:\n",
    "    msg = \"Excelente generalizaci√≥n üíé (modelo estable y sin overfitting).\"\n",
    "elif abs(gap_train_test) < 5:\n",
    "    msg = \"Buena generalizaci√≥n üëç (ligero gap esperable en problemas reales).\"\n",
    "elif gap_train_test > 5:\n",
    "    msg = \"‚ö†Ô∏è Posible overfitting: el modelo rinde mejor en train que en test.\"\n",
    "else:\n",
    "    msg = \"‚ö†Ô∏è Posible underfitting: el modelo no llega a aprender bien los patrones.\"\n",
    "\n",
    "print(f\"\\nüîé Diagn√≥stico: {msg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea67115d",
   "metadata": {},
   "source": [
    "## üß† Interpretaci√≥n de la gr√°fica de generalizaci√≥n\n",
    "\n",
    "La gr√°fica compara la **precisi√≥n (accuracy)** del modelo en los tres conjuntos de datos:\n",
    "\n",
    "- üü© **Entrenamiento (Train)** ‚Üí mide c√≥mo de bien el modelo aprende los patrones conocidos.  \n",
    "- üü™ **Validaci√≥n (Val)** ‚Üí eval√∫a el rendimiento en datos no usados durante el aprendizaje (control del *overfitting*).  \n",
    "- üü¶ **Test** ‚Üí mide la capacidad del modelo para generalizar a datos completamente nuevos.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä C√≥mo interpretar las diferencias\n",
    "\n",
    "| M√©trica | Significado | Interpretaci√≥n |\n",
    "|:--------|:-------------|:---------------|\n",
    "| **Train alto y Val/Test similares** | El modelo aprende bien sin memorizar. | ‚úÖ Buen equilibrio y generalizaci√≥n. |\n",
    "| **Train ‚â´ Val/Test (gap > 5%)** | El modelo memoriza los datos de entrenamiento. | ‚ö†Ô∏è *Overfitting* ‚Äî mala generalizaci√≥n. |\n",
    "| **Train ‚âà Val ‚âà Test pero todos bajos** | El modelo no aprende patrones relevantes. | ‚ö†Ô∏è *Underfitting* ‚Äî poca capacidad o datos insuficientes. |\n",
    "| **Val o Test ligeramente menor (2‚Äì5%)** | Diferencia esperable en la pr√°ctica. | üëç Modelo estable y bien regularizado. |\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Qu√© observar en la gr√°fica\n",
    "\n",
    "- Si las barras de **Validaci√≥n** y **Test** est√°n **cercanas entre s√≠**, el modelo **generaliza correctamente**.  \n",
    "- Si la barra de **Entrenamiento** est√° muy por encima, el modelo **sobreajusta**.  \n",
    "- Si todas las barras est√°n bajas, el modelo **no est√° aprendiendo** los patrones de forma efectiva.  \n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Gap de generalizaci√≥n\n",
    "\n",
    "La diferencia porcentual entre *Train* y *Test* indica el nivel de generalizaci√≥n:\n",
    "\n",
    "| Gap (`train - test`) | Diagn√≥stico |\n",
    "|:---------------------:|:-------------|\n",
    "| **< 2 %** | üíé Excelente generalizaci√≥n |\n",
    "| **2‚Äì5 %** | üëç Buena generalizaci√≥n |\n",
    "| **> 5 %** | ‚ö†Ô∏è Posible *overfitting* |\n",
    "| **Gap negativo grande** | ‚ö†Ô∏è *Underfitting* ‚Äî el modelo rinde peor incluso en entrenamiento |\n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ En resumen\n",
    "\n",
    "> Un modelo **bien generalizado** mantiene un rendimiento similar entre *train*, *val* y *test*,  \n",
    "> lo que indica que ha aprendido **patrones reales** y no solo ha memorizado las im√°genes de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a735fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Celda 8: Curvas de p√©rdida y precisi√≥n (versi√≥n final) ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-v0_8-muted')\n",
    "\n",
    "epochs_range = range(1, len(history[\"train_loss\"]) + 1)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# --- P√©rdida ---\n",
    "ax[0].plot(epochs_range, history[\"train_loss\"], \"o-\", color=\"#1f77b4\", label=\"Entrenamiento\")\n",
    "ax[0].plot(epochs_range, history[\"val_loss\"], \"o--\", color=\"#ff7f0e\", label=\"Validaci√≥n\")\n",
    "ax[0].set_title(\"Evoluci√≥n de la p√©rdida\", fontsize=13)\n",
    "ax[0].set_xlabel(\"√âpoca\"); ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].legend(frameon=False); ax[0].grid(ls=\"--\", alpha=0.6)\n",
    "\n",
    "# --- Precisi√≥n ---\n",
    "ax[1].plot(epochs_range, history[\"train_acc\"], \"o-\", color=\"#57c057\", label=\"Entrenamiento\")\n",
    "ax[1].plot(epochs_range, history[\"val_acc\"], \"o--\", color=\"#c662c1\", label=\"Validaci√≥n\")\n",
    "ax[1].set_title(\"Evoluci√≥n de la precisi√≥n\", fontsize=13)\n",
    "ax[1].set_xlabel(\"√âpoca\"); ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].legend(frameon=False); ax[1].grid(ls=\"--\", alpha=0.6)\n",
    "\n",
    "plt.suptitle(\"Curvas de entrenamiento del modelo\", fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"curvas_entrenamiento.png\", dpi=220, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"‚úÖ Guardadas como: curvas_entrenamiento.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
